{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f1dbfc8-89dd-4da5-9a33-57491c045064",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Change report from Direct Lake to Import Mode\n",
    "\n",
    "This notebook can be used to:\n",
    "1. Extract the BIM file of a Direct Lake file.\n",
    "2. Transform the BIM File to use Import Mode.\n",
    "3. Create a new Semantic Model in Import Mode.\n",
    "4. Get the manual steps needed to prepare the Semantic Model for use.\n",
    "5. Finalise the Semantic Model for use going forwards.\n",
    "6. Bind existing reports to the new model making the old model obsolete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb782e1-d658-43ab-b4fa-ac3dd345bbcd",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Install semantic-link-labs if you haven't already\n",
    "\n",
    "%pip install semantic-link-labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92f0d3b-4194-4ce5-b360-e89bc2847f47",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import sempy_labs as sl\n",
    "import sempy_labs.report as rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f920b147-c5cb-4289-9fe4-68f15f369ecf",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Source details\n",
    "DIRECT_LAKE_MODEL_NAME = 'direct_lake_model_name'\n",
    "WORKSPACE_GUID = 'cbf8bf70-4f5f-4d8a-86eb-5af279801c77'\n",
    " \n",
    "# Target details\n",
    "NEW_MODEL_NAME = 'import_mode_model_name'\n",
    "\n",
    "# SQL endpoint information\n",
    "ENDPOINT = 'drrjn3f71e92a3a783bdmmvui-ro3f71e92a3a785k25c8e106au.datawarehouse.fabric.microsoft.com'\n",
    "LAKEHOUSE_NAME = 'lakehouse_used_by_model'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c795a0ab-c9df-462c-b3a9-6ba727831555",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Create an Import Version of the Semantic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf8bf70-4f5f-4d8a-86eb-5af279801c77",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Get the semantic model definition for the direct lake model from the source workspace\n",
    "model_bim = sl.get_semantic_model_definition(\n",
    "    dataset = DIRECT_LAKE_MODEL_NAME,\n",
    "    format = 'TMSL',\n",
    "    workspace = WORKSPACE_GUID,\n",
    "    return_dataframe = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661671af-b9a2-4f4d-b6c8-83e721901d79",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Change directLake tables to import\n",
    "PBI_QueryOrderList = []\n",
    "\n",
    "for table in model_bim.get('model').get('tables'):\n",
    "\n",
    "    first_partition = table.get('partitions')[0]\n",
    "\n",
    "    if first_partition.get('mode') == 'directLake':\n",
    "        name = table.get('name')\n",
    "        entity = first_partition.get('source').get('entityName')\n",
    "        schema  = first_partition.get('source').get('schemaName') or 'dbo'\n",
    "\n",
    "        new_partition = {\n",
    "            'name': name,\n",
    "            'mode': 'import',\n",
    "            'source': {\n",
    "                'expression': [\n",
    "                    'let',\n",
    "                    '    Source = Sql.Database(\"' + ENDPOINT + '\", \"' + LAKEHOUSE_NAME + '\"),',\n",
    "                    '    navigation = Source{[Schema=\"' + schema + '\",Item=\"' + entity + '\"]}[Data]',\n",
    "                    'in',\n",
    "                    '    navigation'],\n",
    "                'type': 'm'\n",
    "                }\n",
    "        }\n",
    "\n",
    "        PBI_QueryOrderList.append(name)\n",
    "\n",
    "        print(f'Updating {name} to import table')\n",
    "\n",
    "        table.update({'partitions': [new_partition]})\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a313d9-eb9f-4a0f-9984-aabffd67ffcb",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Remove directLake annotations and expressions\n",
    "annotations = model_bim.get('model').get('annotations')\n",
    "\n",
    "for i, annotation in enumerate(annotations):\n",
    "\n",
    "    match annotation.get('name'):\n",
    "\n",
    "        case 'PBI_ProTooling':\n",
    "            annotations.pop(i)\n",
    "    \n",
    "        case 'PBI_QueryOrder':\n",
    "            annotations[i] = {'name': 'PBI_QueryOrder', 'value': f'{PBI_QueryOrderList}'}\n",
    "\n",
    "model_bim.get('model').update({'annotations': annotations})\n",
    "\n",
    "model_bim.get('model').pop('expressions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c20e355-aabf-4d56-9bde-f15c0a5ff65e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Create the new model\n",
    "sl.create_semantic_model_from_bim(\n",
    "    dataset = NEW_MODEL_NAME,\n",
    "    bim_file = model_bim,\n",
    "    workspace = WORKSPACE_GUID\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e470afd-c5fe-438b-93f5-791b03275b3e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Some manual steps\n",
    "\n",
    "Changing to import seems to cause some problems with relationships and sort orders after the first refresh.\n",
    "\n",
    "To get around this and to avoid auto date/time issues causing problems you need to download the PBIX to Desktop for the first refresh and then re-import. This will delete all relationships and sort orders, but we can recreate them and then they will work as normal.\n",
    "\n",
    "To do this open the workspace where the Semantic Model has been created. Find the model > Download this file and open it in Power BI Desktop. In Power BI Desktop:\n",
    "\n",
    "\n",
    "1. Open the Semantic Model > **File** > **Options & Setting** > **Options** > **Current File** > **Data Load**:\n",
    "    1. In **Data Load** > Turn off all **Relationship settings**\n",
    "    1. In **Data Load** > Turn off **Auto date/time**\n",
    "1. Refresh the Power BI Model in Desktop. You will notice that existing relationships may have been deleted. This is expected. All tables should be unrelated to each other.\n",
    "1. Publish the model to Power BI Service from desktop.\n",
    "\n",
    "Then in Power BI Service:\n",
    "\n",
    "2. Find the Semantic Model in the Workspace > open its **Settings** > Go to **Gateway and cloud connections** > Create a Connection for the SQL Endpoint.\n",
    "3. Refresh the Semantic Model\n",
    "4. Open the new Semantic Model > switch from **View** to **Edit** mode > Refresh in the editor\n",
    "\n",
    "No auto-detected relationships should be created, but the refresh should be successful. This confirms that the import mode is working.\n",
    "\n",
    "Now we need to create the Relationships and the sort order of the columns again, and bind the existing reports to the new model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f5f0b9",
   "metadata": {},
   "source": [
    "# Recreate the Relationships and Sort Orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9864d2af-1157-4be7-902e-7a4ace4f7674",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Get the new semantic model\n",
    "new_model_bim = sl.get_semantic_model_definition(\n",
    "    dataset = NEW_MODEL_NAME,\n",
    "    format = 'TMSL',\n",
    "    workspace = WORKSPACE_GUID,\n",
    "    return_dataframe = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f6f240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the sort by columns in the original semantic model\n",
    "sort_by_columns = {}\n",
    "\n",
    "for table in model_bim.get('model').get('tables'):\n",
    "\n",
    "    columns = {}\n",
    "\n",
    "    for column in table.get('columns'):\n",
    "        if column.get('sortByColumn'):\n",
    "            columns.update({column.get('name'): column.get('sortByColumn')})\n",
    "\n",
    "    if columns != {}: \n",
    "        sort_by_columns.update({table.get('name'): columns})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40629f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the new model's sortByColumn(s) to be the Direct Lake model's sort by columns\n",
    "for table in new_model_bim.get('model').get('tables'):\n",
    "    \n",
    "    table_name = sort_by_columns.get(table.get('name'))\n",
    "    \n",
    "    # If there are sort by columns to update:\n",
    "    if table_name:\n",
    "        for column in table.get('columns'):\n",
    "\n",
    "            to_sort_by_column = table_name.get(column.get('name'))\n",
    "            \n",
    "            if to_sort_by_column:\n",
    "                column.update({'sortByColumn': to_sort_by_column})\n",
    "                print(f'Mapped: {column.get('name')}    ->  {to_sort_by_column}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2086ae1b-384c-4706-8f08-3cd9c3dd4b18",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Set the new model's relationships to be the direct lake model's relationships\n",
    "new_model_bim.get('model').update({'relationships': model_bim.get('model').get('relationships')})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0d69be-3bd3-4d84-b77a-bf6212ab3f58",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Bind Reports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc3ef3e-e23f-4cf2-a927-87f50557c455",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Get all reports linked to the original report to bind to new model\n",
    "reports = sl.list_reports_using_semantic_model(DIRECT_LAKE_MODEL_NAME, WORKSPACE_GUID)\n",
    "reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b50675-c267-444d-846b-f0deda871a56",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Rebind each report to the new semantic model\n",
    "for report in reports['Report Id']:\n",
    "    rep.report_rebind(\n",
    "        report = report,\n",
    "        dataset = NEW_MODEL_NAME,\n",
    "        report_workspace = WORKSPACE_GUID,\n",
    "        dataset_workspace = WORKSPACE_GUID\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "dependencies": {},
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
